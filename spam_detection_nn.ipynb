{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f99b45d5-7d8f-47a1-b6e8-b7506148fc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               email  label  \\\n",
      "0   事業者 氏名 vip mail 突然のメール失礼いたします 今後この広告がご不要な方はその...      1   \n",
      "1  there is no stumbling on to it the greatest wa...      1   \n",
      "2  we thought you may be interested in our new so...      1   \n",
      "3   take control of your computer with this top o...      1   \n",
      "4  me and my friends have this brand new idea a l...      1   \n",
      "\n",
      "                                          clean_text  \n",
      "0     vip mail   stop vip url  vip mail vip mail ...  \n",
      "1  there is no stumbling on to it the greatest wa...  \n",
      "2  we thought you may be interested in our new so...  \n",
      "3   take control of your computer with this top o...  \n",
      "4  me and my friends have this brand new idea a l...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Define the preprocess_text function\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Ensure the text is a string\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "        return text\n",
    "    return \"\"  # Return an empty string for non-string values\n",
    "\n",
    "# Apply preprocessing to the 'email' column\n",
    "data['clean_text'] = data['email'].apply(preprocess_text).fillna(\"\")\n",
    "\n",
    "# Balancing the dataset\n",
    "spam = data[data['label'] == 1]\n",
    "not_spam = data[data['label'] == 0]\n",
    "\n",
    "# Balance the data by undersampling\n",
    "min_count = min(len(spam), len(not_spam))\n",
    "balanced_data = pd.concat([spam.sample(min_count), not_spam.sample(min_count)]).reset_index(drop=True)\n",
    "\n",
    "print(balanced_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e692e11f-931a-46b1-b68f-5d8decd3624f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 4993843.3960\n",
      "Epoch 2/5, Loss: 3688138.3942\n",
      "Epoch 3/5, Loss: 2841609.8187\n",
      "Epoch 4/5, Loss: 2303532.5609\n",
      "Epoch 5/5, Loss: 2108654.7788\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the clean_text column\n",
    "data['tokens'] = data['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = Counter(word for tokens in data['tokens'] for word in tokens)\n",
    "vocab_size = len(vocab)\n",
    "word_to_index = {word: i for i, word in enumerate(vocab.keys())}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_size = 10  # Embedding size for Word2Vec\n",
    "window_size = 2  # Context window size\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "negative_samples = 5\n",
    "\n",
    "# Initialize embeddings\n",
    "word_embeddings = np.random.randn(vocab_size, embedding_size)\n",
    "context_embeddings = np.random.randn(vocab_size, embedding_size)\n",
    "\n",
    "# Generate training data for Word2Vec\n",
    "def generate_training_data(tokens, window_size):\n",
    "    training_data = []\n",
    "    for sentence in tokens:\n",
    "        for i, word in enumerate(sentence):\n",
    "            target = word_to_index[word]\n",
    "            context = []\n",
    "            for j in range(-window_size, window_size + 1):\n",
    "                if j != 0 and 0 <= i + j < len(sentence):\n",
    "                    context.append(word_to_index[sentence[i + j]])\n",
    "            training_data.append((target, context))\n",
    "    return training_data\n",
    "\n",
    "training_data = generate_training_data(data['tokens'], window_size)\n",
    "\n",
    "# Negative sampling function\n",
    "def negative_sampling(vocab_size, positive_indices, num_samples):\n",
    "    negatives = []\n",
    "    while len(negatives) < num_samples:\n",
    "        neg = np.random.randint(0, vocab_size)\n",
    "        if neg not in positive_indices:\n",
    "            negatives.append(neg)\n",
    "    return negatives\n",
    "\n",
    "# Training the Word2Vec model\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for target, context in training_data:\n",
    "        positive_indices = context\n",
    "        negative_indices = negative_sampling(vocab_size, positive_indices, negative_samples)\n",
    "        \n",
    "        # Calculate gradients and update weights\n",
    "        for context_word in positive_indices + negative_indices:\n",
    "            label = 1 if context_word in positive_indices else 0\n",
    "            dot_product = np.dot(word_embeddings[target], context_embeddings[context_word])\n",
    "            prediction = 1 / (1 + np.exp(-dot_product))\n",
    "            error = label - prediction\n",
    "            \n",
    "            # Update weights\n",
    "            word_embeddings[target] += learning_rate * error * context_embeddings[context_word]\n",
    "            context_embeddings[context_word] += learning_rate * error * word_embeddings[target]\n",
    "            \n",
    "            loss += -label * np.log(prediction + 1e-9) - (1 - label) * np.log(1 - prediction + 1e-9)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Save the final word embeddings\n",
    "word_to_vec = {word: word_embeddings[word_to_index[word]] for word in vocab.keys()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adc8abee-bee0-4c9e-864a-489bda0930d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.2520\n",
      "Epoch 2/50, Loss: 0.1677\n",
      "Epoch 3/50, Loss: 0.1673\n",
      "Epoch 4/50, Loss: 0.1664\n",
      "Epoch 5/50, Loss: 0.1640\n",
      "Epoch 6/50, Loss: 0.1574\n",
      "Epoch 7/50, Loss: 0.1480\n",
      "Epoch 8/50, Loss: 0.1440\n",
      "Epoch 9/50, Loss: 0.1426\n",
      "Epoch 10/50, Loss: 0.1422\n",
      "Epoch 11/50, Loss: 0.1421\n",
      "Epoch 12/50, Loss: 0.1420\n",
      "Epoch 13/50, Loss: 0.1420\n",
      "Epoch 14/50, Loss: 0.1420\n",
      "Epoch 15/50, Loss: 0.1419\n",
      "Epoch 16/50, Loss: 0.1419\n",
      "Epoch 17/50, Loss: 0.1419\n",
      "Epoch 18/50, Loss: 0.1419\n",
      "Epoch 19/50, Loss: 0.1418\n",
      "Epoch 20/50, Loss: 0.1418\n",
      "Epoch 21/50, Loss: 0.1417\n",
      "Epoch 22/50, Loss: 0.1417\n",
      "Epoch 23/50, Loss: 0.1417\n",
      "Epoch 24/50, Loss: 0.1416\n",
      "Epoch 25/50, Loss: 0.1416\n",
      "Epoch 26/50, Loss: 0.1415\n",
      "Epoch 27/50, Loss: 0.1415\n",
      "Epoch 28/50, Loss: 0.1414\n",
      "Epoch 29/50, Loss: 0.1413\n",
      "Epoch 30/50, Loss: 0.1413\n",
      "Epoch 31/50, Loss: 0.1412\n",
      "Epoch 32/50, Loss: 0.1411\n",
      "Epoch 33/50, Loss: 0.1410\n",
      "Epoch 34/50, Loss: 0.1408\n",
      "Epoch 35/50, Loss: 0.1407\n",
      "Epoch 36/50, Loss: 0.1405\n",
      "Epoch 37/50, Loss: 0.1403\n",
      "Epoch 38/50, Loss: 0.1401\n",
      "Epoch 39/50, Loss: 0.1398\n",
      "Epoch 40/50, Loss: 0.1395\n",
      "Epoch 41/50, Loss: 0.1392\n",
      "Epoch 42/50, Loss: 0.1389\n",
      "Epoch 43/50, Loss: 0.1386\n",
      "Epoch 44/50, Loss: 0.1382\n",
      "Epoch 45/50, Loss: 0.1379\n",
      "Epoch 46/50, Loss: 0.1375\n",
      "Epoch 47/50, Loss: 0.1371\n",
      "Epoch 48/50, Loss: 0.1368\n",
      "Epoch 49/50, Loss: 0.1364\n",
      "Epoch 50/50, Loss: 0.1360\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.weights_input_hidden = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.weights_hidden_output = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.bias_hidden = np.zeros((1, hidden_size))\n",
    "        self.bias_output = np.zeros((1, output_size))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.hidden_input = np.dot(x, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = self.sigmoid(self.hidden_input)\n",
    "        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.final_output = self.sigmoid(self.final_input)\n",
    "        return self.final_output\n",
    "    \n",
    "    def backward(self, x, y, output, learning_rate):\n",
    "        output_error = y - output\n",
    "        output_delta = output_error * self.sigmoid_derivative(output)\n",
    "        \n",
    "        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * learning_rate\n",
    "        self.weights_input_hidden += np.dot(x.T, hidden_delta) * learning_rate\n",
    "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "# Prepare data for the neural network\n",
    "def prepare_input_data(tokens, word_to_vec, max_words=12, embedding_size=10):\n",
    "    inputs = []\n",
    "    for sentence in tokens:\n",
    "        embeddings = [word_to_vec[word] for word in sentence if word in word_to_vec]\n",
    "        if len(embeddings) < max_words:\n",
    "            embeddings += [np.zeros(embedding_size)] * (max_words - len(embeddings))\n",
    "        inputs.append(np.array(embeddings[:max_words]).flatten())\n",
    "    return np.array(inputs)\n",
    "\n",
    "X = prepare_input_data(data['tokens'], word_to_vec)\n",
    "y = data['label'].values.reshape(-1, 1)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the neural network\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 8\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    output = nn.forward(X_train)\n",
    "    nn.backward(X_train, y_train, output, learning_rate)\n",
    "    loss = np.mean((y_train - output) ** 2)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "666602b9-cf0b-494f-aa80-f9da5c3ffd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.00%\n",
      "\n",
      "Classification Report:\n",
      "Label 0:\n",
      "  precision: 0.9619\n",
      "  recall: 0.9897\n",
      "  f1_score: 0.9756\n",
      "  support: 485.0000\n",
      "Label 1:\n",
      "  precision: 0.9505\n",
      "  recall: 0.8348\n",
      "  f1_score: 0.8889\n",
      "  support: 115.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from math import log2\n",
    "\n",
    "# Function to preprocess and clean the text\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):  # Check if the text is not a string (e.g., NaN, float)\n",
    "        return ''  # Return an empty string or handle it as needed\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove non-alphabetic characters (only keep letters and spaces)\n",
    "    return text\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(r\"\")  # Update with your actual path\n",
    "\n",
    "# Preprocess the 'email' column in the dataset\n",
    "data['clean_text'] = data['email'].apply(preprocess_text)  # Clean the text in the 'email' column\n",
    "\n",
    "# Split the dataset into features (X) and target labels (y)\n",
    "X = data['clean_text']  # Features (cleaned text)\n",
    "y = data['label']  # Target labels (0 for ham, 1 for spam)\n",
    "\n",
    "# Split the data into training and testing sets manually\n",
    "# Randomly shuffle the data and split\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(X))\n",
    "train_size = int(0.8 * len(X))\n",
    "train_indices, test_indices = indices[:train_size], indices[train_size:]\n",
    "\n",
    "X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
    "\n",
    "# Manual Text Vectorization: Bag of Words\n",
    "def build_vocab(corpus):\n",
    "    vocab = Counter()\n",
    "    for text in corpus:\n",
    "        vocab.update(text.split())\n",
    "    return vocab\n",
    "\n",
    "def vectorize_text(text, vocab):\n",
    "    # Convert text into a vector based on the vocabulary (Bag of Words)\n",
    "    text_vector = np.zeros(len(vocab))\n",
    "    words = text.split()\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    for idx, word in enumerate(vocab):\n",
    "        if word in word_counts:\n",
    "            text_vector[idx] = word_counts[word]\n",
    "    \n",
    "    return text_vector\n",
    "\n",
    "# Build vocabulary on the training data\n",
    "vocab = build_vocab(X_train)\n",
    "\n",
    "# Vectorize the training and test data\n",
    "X_train_vec = np.array([vectorize_text(text, vocab) for text in X_train])\n",
    "X_test_vec = np.array([vectorize_text(text, vocab) for text in X_test])\n",
    "\n",
    "# Implementing a simple Decision Tree Classifier (for binary classification)\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "        \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        if len(set(y)) == 1:  # If all labels are the same# Instead of y[0], use y.iloc[0] or y.values[0] (for NumPy array)\n",
    "            return y.iloc[0]  # or return y.values[0]\n",
    "\n",
    "        if depth >= self.max_depth:  # Max depth reached\n",
    "            return self._majority_vote(y)\n",
    "        \n",
    "        best_split = self._find_best_split(X, y)\n",
    "        if not best_split:\n",
    "            return self._majority_vote(y)\n",
    "        \n",
    "        left_indices = X[:, best_split['feature']] <= best_split['value']\n",
    "        right_indices = ~left_indices\n",
    "        \n",
    "        left_tree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_tree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        \n",
    "        return {\n",
    "            'feature': best_split['feature'],\n",
    "            'value': best_split['value'],\n",
    "            'left': left_tree,\n",
    "            'right': right_tree\n",
    "        }\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_gini = float('inf')\n",
    "        best_split = None\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        for feature in range(n_features):\n",
    "            values = np.unique(X[:, feature])\n",
    "            for value in values:\n",
    "                left_indices = X[:, feature] <= value\n",
    "                right_indices = ~left_indices\n",
    "                left_y, right_y = y[left_indices], y[right_indices]\n",
    "                \n",
    "                gini_left = self._gini_impurity(left_y)\n",
    "                gini_right = self._gini_impurity(right_y)\n",
    "                \n",
    "                gini = (len(left_y) * gini_left + len(right_y) * gini_right) / len(y)\n",
    "                \n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_split = {'feature': feature, 'value': value}\n",
    "        \n",
    "        return best_split\n",
    "    \n",
    "    def _gini_impurity(self, y):\n",
    "        classes = np.unique(y)\n",
    "        impurity = 1\n",
    "        for c in classes:\n",
    "            prob = np.sum(y == c) / len(y)\n",
    "            impurity -= prob**2\n",
    "        return impurity\n",
    "    \n",
    "    def _majority_vote(self, y):\n",
    "        return np.bincount(y).argmax()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [self._predict_single(x, self.tree) for x in X]\n",
    "\n",
    "    def _predict_single(self, x, tree):\n",
    "        if isinstance(tree, dict):\n",
    "            if x[tree['feature']] <= tree['value']:\n",
    "                return self._predict_single(x, tree['left'])\n",
    "            else:\n",
    "                return self._predict_single(x, tree['right'])\n",
    "        return tree\n",
    "\n",
    "# Train the DecisionTree model\n",
    "model = DecisionTree(max_depth=10)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print a simple classification report\n",
    "from collections import Counter\n",
    "def classification_report_manual(y_true, y_pred):\n",
    "    # Calculate Precision, Recall, F1-Score for both classes\n",
    "    report = {}\n",
    "    labels = np.unique(y_true)\n",
    "    for label in labels:\n",
    "        tp = sum((y_true == label) & (y_pred == label))\n",
    "        fp = sum((y_true != label) & (y_pred == label))\n",
    "        fn = sum((y_true == label) & (y_pred != label))\n",
    "        tn = sum((y_true != label) & (y_pred != label))\n",
    "        \n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        report[label] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "            'support': tp + fn\n",
    "        }\n",
    "    return report\n",
    "\n",
    "report = classification_report_manual(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\")\n",
    "for label, metrics in report.items():\n",
    "    print(f\"Label {label}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc3b20-2663-474f-8954-51b24bd564c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f649fdd1-9580-4e3b-81a2-ee5fbe1a519c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
